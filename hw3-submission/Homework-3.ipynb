{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Hotel Ratings on Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will focus on practicing two techniques: web scraping and regression. For the first part, we will get some basic information for each hotel in Boston. Then, we will fit a regression model on this information and try to analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1 (30 pts)**\n",
    "\n",
    "We will scrape the data using Beautiful Soup. For each hotel that our search returns, we will get the information below.\n",
    "\n",
    "![Information to be scraped](hotel_info.png)\n",
    "\n",
    "Of course, feel free to collect even more data if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " get url\n",
      "URL TO REQUEST: http://www.tripadvisor.com/TypeAheadJson?query=boston%20massachusetts&action=API\n",
      "RESULTS:  {u'lookbackServlet': None, u'name': u'Boston, Massachusetts, United States', u'data_type': u'LOCATION', u'title': u'Destinations', u'url': u'/Tourism-g60745-Boston_Massachusetts-Vacations.html', u'value': 60745, u'coords': u'42.357277,-71.05834', u'urls': [{u'url': u'/Tourism-g60745-Boston_Massachusetts-Vacations.html', u'type': u'GEO', u'name': u'Boston Tourism', u'url_type': u'geo'}], u'scope': u'global', u'type': u'GEO'}\n",
      "CITY PAGE URL:"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "import collections\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "base_url = \"http://www.tripadvisor.com\"\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\"\n",
    "\n",
    "\"\"\" STEP 1  \"\"\"\n",
    "def get_tourism_page(city, state):\n",
    "    \"\"\" \n",
    "        Return the json containing the\n",
    "        URL of the tourism city page\n",
    "    \"\"\"\n",
    "\n",
    "    # EXAMPLE: http://www.tripadvisor.com/TypeAheadJson?query=boston%20massachusetts&action=API\n",
    "    #          http://www.tripadvisor.com//TypeAheadJson?query=san%20francisco%20california&type=GEO&action=API\n",
    "    url = \"%s/TypeAheadJson?query=%s%%20%s&action=API\" % (base_url, \"%20\".join(city.split()), state)\n",
    "    print \"URL TO REQUEST:\", url\n",
    "    \n",
    "    # Given the url, request the HTML page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "\n",
    "    # Save to file\n",
    "    #with open('search-page.json', \"w\") as h:\n",
    "        #h.write(html)\n",
    "\n",
    "    # Parse json to get url\n",
    "    js = json.loads(html)\n",
    "    results = js['results']\n",
    "    print \"RESULTS: \", results[0]\n",
    "    urls = results[0]['urls'][0]\n",
    "\n",
    "    # get tourism page url\n",
    "    tourism_url = urls['url']\n",
    "    return tourism_url\n",
    "\n",
    "\"\"\" STEP 2  \"\"\"\n",
    "def get_city_page(tourism_url):\n",
    "    \"\"\" \n",
    "        Get the URL of the hotels of the city\n",
    "        using the URL returned by the function\n",
    "        get_tourism_page()\n",
    "    \"\"\"\n",
    "\n",
    "    url = base_url + tourism_url\n",
    "\n",
    "    # Given the url, request the HTML page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    \n",
    "    # Save to file\t\n",
    "    #with open('-tourism-page.html', \"w\") as h:\n",
    "        #h.write(html)\n",
    "\n",
    "\n",
    "    # Use BeautifulSoup to extract the url for the list of hotels in \n",
    "    # the city and state we are interested in.\n",
    "    # For exampel in this case we need to  \n",
    "    #<li class=\"hotels twoLines\">\n",
    "    #<a href=\"/Hotels-g60745-Boston_Massachusetts-Hotels.html\" data-trk=\"hotels_nav\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    li = soup.find(\"li\", {\"class\": \"hotels twoLines\"})\n",
    "    city_url = li.find('a', href = True)\n",
    "    print \"CITY PAGE URL:\", city_url['href']\n",
    "    return city_url['href']\n",
    "\n",
    "\n",
    "\"\"\" STEP 3 \"\"\"\n",
    "def get_hotellist_page(city_url, count):\n",
    "    \"\"\" Get the hotel list page given the url returned by\n",
    "        get_city_page(). Return the html after saving\n",
    "        it to the datadir \n",
    "    \"\"\"\n",
    "    print \"Hotel page\", count\n",
    "    url = base_url + city_url\n",
    "    # Sleep 2 sec before starting a new http request\n",
    "    time.sleep(2)\n",
    "    # Request page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    # Save the \n",
    "    #with open('boston-hotelist-' + str(count) + '.html', \"w\") as h:\n",
    "        #h.write(html)\n",
    "    return html\n",
    "\n",
    "\"\"\" STEP 4 \"\"\"\n",
    "def parse_hotellist_page(html):\n",
    "    \"\"\" \n",
    "    Parse the html pages returned by get_hotellist_page().\n",
    "    Return the next url page to scrape (a city can have\n",
    "    more than one page of hotels) if there is, else exit\n",
    "    the script.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "# Extract hotel name, star rating and number of reviews\n",
    "    hotel_boxes = soup.findAll('div', {'class' :'listing easyClear  p13n_imperfect '})\n",
    "    print len(hotel_boxes), \"hotels\"\n",
    "    for hotel_box in hotel_boxes:\n",
    "        name = hotel_box.find('div', {'class' :'listing_title'}).find(text=True)\n",
    "        try:\n",
    "            rating = hotel_box.find('div', {'class' :'listing_rating'})\n",
    "            reviews = rating.find('span', {'class' :'more'}).find(text=True)\n",
    "            stars = hotel_box.find(\"img\", {\"class\" : \"sprite-ratings\"})\n",
    "        except Exception, e:\n",
    "            print \"no ratings for\", name\n",
    "            reviews = \"N/A\"\n",
    "            stars = 'N/A'\n",
    "        hotelref = hotel_box.findAll('a', href= True)\n",
    "        #print \"go to \", hotelref[0]['href'],\" and get traveler ratings\"\n",
    "        print name,\n",
    "        ratingfile.write(\"++NEW HOTEL++ %s\\n\" % name)\n",
    "        print '.',\n",
    "        getTraverlerRating(hotelref[0]['href'])\n",
    "        \n",
    "        \n",
    "        if stars != 'N/A':\n",
    "            #log.info(\"Stars: %s\" % stars['alt'].split()[0])\n",
    "            stars = stars['alt'].split()[0]\n",
    "        if name == \"Omni Parker House\":\n",
    "            print \"Found Omni Parker House. Scrape reviews\"\n",
    "            print \"HOTEL NAME:\", name\n",
    "            print \"HOTEL REVIEWS: \", reviews\n",
    "            print \"HOTEL STAR RATING:\", stars\n",
    "            omnihrefs = hotel_box.findAll('a', href= True)\n",
    "            for omnihref in omnihrefs:\n",
    "                #print omnihref, \"######\", omnihref['href']\n",
    "                if omnihref.find(text = True) == 'Omni Parker House':\n",
    "                    \n",
    "                    pg = 0\n",
    "                    #print \"Review url is\", omnihref['href']\n",
    "                    #print \"page #\", pg,\n",
    "                    #REMOVE COMMENT ret = scrapeReview(omnihref['href'], pg)\n",
    "                    #ret = scrapeFaster(omnihref['href'])                    \n",
    "                    ret = None\n",
    "                    while ret:\n",
    "                        pg +=1\n",
    "                        print \"page #\", pg, \n",
    "                        #REMOVE COMMNET ret = scrapeReview(ret, pg)\n",
    "                    print \"Done scraping omni hotel\"\n",
    "                    \"\"\"\n",
    "                    print \"Review url begin:\", omnihref['href'] \n",
    "                    scrapeReview(omnihref['href'])\n",
    "                    \"\"\"\n",
    "                    #add this block in main flow to scrape everything\n",
    "                #return\n",
    "\n",
    "# # Get next URL page if exists, else exit\n",
    "    div = soup.find(\"div\", {\"class\" : \"unified pagination standard_pagination\"})\n",
    "    # check if last page\n",
    "    if div.find('span', {'class' : 'nav next ui_button disabled'}):\n",
    "        print \"\\nReached last page\"\n",
    "        return None\n",
    "    # If it is not las page there must be the Next URL\n",
    "    hrefs = div.findAll('a', href= True)\n",
    "    for href in hrefs:\n",
    "        if href.find(text = True) == 'Next':\n",
    "            print \"Next url is\", href['href']\n",
    "            return href['href']\n",
    "\n",
    "\"\"\"Get Traverler's ratings for every hotel\"\"\"\n",
    "def getTraverlerRating(hotelurl):\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+hotelurl, headers=headers)\n",
    "    #print response\n",
    "    html = response.text.encode('utf-8')   \n",
    "    hotelsoup = BeautifulSoup(html)\n",
    "    \n",
    "    try: \n",
    "        filterbox = hotelsoup.findAll(\"div\",{\"class\":\"with_histogram\"})\n",
    "        ratebox = filterbox[0].findAll(\"div\",{\"class\":\"col rating \"})\n",
    "        ratinglist = ratebox[0].findAll(\"li\")\n",
    "        excel = ratinglist[0].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_5\"})[0]\n",
    "        ratingfile.write(\"Excellent:%s\\n\" % excel.findAll(\"span\")[2].find(text=True))\n",
    "        #print \"excel\", excel.findAll(\"span\")[2].find(text=True), \n",
    "        vgood = ratinglist[1].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_4\"})[0]\n",
    "        ratingfile.write(\"Very good: %s\\n\" % vgood.findAll(\"span\")[2].find(text=True))\n",
    "\n",
    "        avg = ratinglist[2].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_3\"})[0]\n",
    "        ratingfile.write(\"Average:%s\\n\" % avg.findAll(\"span\")[2].find(text=True))\n",
    "\n",
    "        poor = ratinglist[3].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_2\"})[0]\n",
    "        ratingfile.write(\"Poor:%s\\n\" % poor.findAll(\"span\")[2].find(text=True))\n",
    "\n",
    "        terrible = ratinglist[4].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_1\"})[0]\n",
    "        ratingfile.write(\"Terrible:%s\\n\" % terrible.findAll(\"span\")[2].find(text=True))\n",
    "\n",
    "        typebox = filterbox[0].findAll(\"div\",{\"class\":\"col segment \"})\n",
    "        typelist = typebox[0].findAll(\"li\")\n",
    "\n",
    "        ratingfile.write(\"Traveler type: \")\n",
    "        #print 'travel'\n",
    "        for t in typelist:\n",
    "            typevallist = t.findAll(\"span\")[1].find(text=True)\n",
    "            #print typevallist,\n",
    "            ratingfile.write(\"%s, \" % typevallist)\n",
    "        ratingfile.write(\"\\n\")\n",
    "        #print \".\"\n",
    "        \n",
    "    except IndexError:\n",
    "        print \"no rating for\", base_url+hotelurl\n",
    "        return\n",
    "    \n",
    "    \n",
    "     \n",
    "    #sys.exit()\n",
    "        \n",
    "\"\"\"STEP 5: Go through each review\"\"\"   \n",
    "\"\"\"\n",
    "def scrapeFaster(url):\n",
    "    driver.get(base_url+url)\n",
    "    \n",
    "    pagehtml = driver.page_source\n",
    "    pgsoup = BeautifulSoup(pagehtml)\n",
    "    try:\n",
    "        nexturl = driver.find_element_by_link_text(\"More\")\n",
    "        print \"More BUTTON\", nexturl\n",
    "    except NoSuchElementException:\n",
    "        print \"NO LINK\"\n",
    "        return\n",
    "    nexturl.click() \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    print \"page loaded\"\n",
    "\"\"\"    \n",
    "\n",
    "\n",
    "def scrapeReview(reviewurl, pgnum):\n",
    "    #return\n",
    "    #print base_url+reviewurl\n",
    "    # debug pupose reviewurl = globalurl\n",
    "    debugfile.write(\"\\nscrapeReview: url %s,\" % base_url+reviewurl)\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+reviewurl, headers=headers)\n",
    "    #print response\n",
    "    debugfile.write(\"scrapeReview: response %s\\n\" % response)\n",
    "    html = response.text.encode('utf-8')   \n",
    "    reviewsoup = BeautifulSoup(html) \n",
    "    \n",
    "    revbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector   track_back\"})\n",
    "    olderrevbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector  \"})\n",
    "    oldestbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector  first_aph   track_back\"})\n",
    "    \n",
    "    #if len(olderrevbox):\n",
    "        #print \"older reviews\", len(olderrevbox)\n",
    "    debugfile.write(\"scrapeReview: total reviews to be parsed %s\\n\" % str(len(revbox)+len(olderrevbox)+len(oldestbox)))\n",
    "    print \"(\",len(revbox)+len(olderrevbox)+len(oldestbox),\")|\",\n",
    "    pg = 1\n",
    "    revbox += olderrevbox+oldestbox\n",
    "    \n",
    "    #click on more button and send expanded cells to getstars2 one-by-one?\n",
    "    for r in revbox:\n",
    "        reviews = r.findAll('a', href=True)        \n",
    "        for rev in reviews:            \n",
    "            thisrevurl = rev['href']\n",
    "            #print thisrevurl\n",
    "            #now make http request for review url and write values to a file\n",
    "            getStars2(thisrevurl)\n",
    "            \n",
    "    #debug purpose sys.exit()\n",
    "    \n",
    "    \"\"\"\n",
    "    reviews = revbox[0].findAll('a', href=True)\n",
    "    thisurl = reviews[0]['href']\n",
    "    nextpageret = getStars(thisurl,0)\n",
    "    \"\"\"\n",
    "    \n",
    "    #nextpages = reviewsoup.findAll(\"a\", {\"class\":\"pageNum taLnk\"})\n",
    "    \"\"\"\n",
    "    print \"next review page\", nextpageret\n",
    "    while nextpageret:        \n",
    "        pg +=1\n",
    "        print \"Next review page #\", pg        \n",
    "        #nextpageret = getStars(nextpageret, pg)\n",
    "    \"\"\"\n",
    "    nextpages = reviewsoup.findAll(\"a\", {\"class\":\"pageNum taLnk\"})\n",
    "    pgnum = min(pgnum,4)\n",
    "    try:\n",
    "        #print \"\\nnext page?\", nextpages[pgnum]['href']\n",
    "        debugfile.write(\"scrapeReview: next page url %s\\n\" % nextpages[pgnum]['href'])\n",
    "        return nextpages[pgnum]['href']\n",
    "    except IndexError:\n",
    "        print \"Done with all pages\", pgnum\n",
    "        return None\n",
    "\n",
    "\"\"\"STEP 6 : Access individual review, parse ratings and store in a file\"\"\"\n",
    "def getStars2(revurl):\n",
    "    #print base_url+revurl,\n",
    "    debugfile.write(\"getStars2: url %s,\" % base_url+revurl)\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+revurl, headers=headers)\n",
    "    #print response\n",
    "    debugfile.write(\"getStars2: response %s\\n\" % response)\n",
    "    html = response.text.encode('utf-8') \n",
    "    \n",
    "    reviewsoup = BeautifulSoup(html)\n",
    "    reviewblock = reviewsoup.findAll(\"div\",{\"class\":\"deckC\"})\n",
    "    try:\n",
    "        reviewlist = reviewblock[0].findAll(\"div\",{\"class\":\"  reviewSelector \"})\n",
    "    except IndexError:\n",
    "        return\n",
    "    review = reviewlist[0]\n",
    "    \n",
    "            \n",
    "    #print review\n",
    "    id = review['id']\n",
    "    #print id, \n",
    "    debugfile.write(\"getStars2: id: %s\\t\" % id)\n",
    "    ratelist = review.findAll(\"div\", {\"class\":\"rating-list\"})\n",
    "    #print ratelist\n",
    "    try:\n",
    "        stars = ratelist[0].findAll(\"li\",{\"class\":\"recommend-answer\"})\n",
    "    except IndexError:\n",
    "        return\n",
    "    #inside stars, access sprite and description and write\n",
    "    #print stars\n",
    "    #ratedict = collections.defaultdict(list)\n",
    "    for val in stars:\n",
    "        v = val.findAll(\"img\")\n",
    "        k = val.findAll(\"div\",{\"class\":\"recommend-description\"})\n",
    "        #print k,v\n",
    "        #print id, \":\",k[0].find(text=True),\":\", v[0]['alt'][0]\n",
    "        #ratedict[k[0].find(text=True)] = v[0]['alt'][0]\n",
    "\n",
    "        try:\n",
    "            access = k[0],v[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        omnifile.write(\"%s:\" % id)\n",
    "        omnifile.write(\"%s:\" % k[0].find(text=True))\n",
    "        omnifile.write(\"%s\\n\" % v[0]['alt'][0])\n",
    "    #nextpage = reviewsoup.findAll(\"a\",{\"class\":\"pageNum taLnk\"})\n",
    "    #print \"go to next\", nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "\"\"\"\n",
    "#Try using selenium. Couldn't get this to work correctly - next page not loading\n",
    "def getStars(revurl, pg):\n",
    "    print base_url+revurl\n",
    "    driver.get(base_url+revurl)\n",
    "    print \"page #\", pg\n",
    "    #time.sleep(1)\n",
    "    while True:\n",
    "        #geturl = base_url+revurl\n",
    "        #headers = { 'User-Agent' : user_agent }\n",
    "        #response = requests.get(base_url+revurl, headers=headers)\n",
    "        #print response\n",
    "        #html = response.text.encode('utf-8')   \n",
    "        \n",
    "        html = driver.page_source\n",
    "        reviewsoup = BeautifulSoup(html) \n",
    "        reviewblock = reviewsoup.findAll(\"div\",{\"class\":\"deckC\"})\n",
    "        reviewlist = reviewblock[0].findAll(\"div\",{\"class\":\"  reviewSelector \"})\n",
    "        #print reviewlist\n",
    "\n",
    "        revnum = 0\n",
    "        for review in reviewlist:        \n",
    "            \n",
    "            if pg and not revnum:\n",
    "                revnum += 1\n",
    "                continue\n",
    "            \n",
    "            #print review\n",
    "            id = review['id']\n",
    "            print id\n",
    "            ratelist = review.findAll(\"div\", {\"class\":\"rating-list\"})\n",
    "            #print ratelist\n",
    "            for i in xrange(len(ratelist)):\n",
    "\n",
    "                stars = ratelist[i].findAll(\"li\",{\"class\":\"recommend-answer\"})\n",
    "                #inside stars, access sprite and description and write\n",
    "                #print stars\n",
    "                #ratedict = collections.defaultdict(list)\n",
    "                for val in stars:\n",
    "                    v = val.findAll(\"img\")\n",
    "                    k = val.findAll(\"div\",{\"class\":\"recommend-description\"})\n",
    "                    #print k,v\n",
    "                    #print id, \":\",k[0].find(text=True),\":\", v[0]['alt'][0]\n",
    "                    #ratedict[k[0].find(text=True)] = v[0]['alt'][0]\n",
    "\n",
    "                    #omnifile.write(\"%s:\" % id)\n",
    "                    #omnifile.write(\"%s:\" % k[0].find(text=True))\n",
    "                    #omnifile.write(\"%s\\n\" % v[0]['alt'][0])\n",
    "            revnum += 1\n",
    "            \n",
    "        pg += 1\n",
    "        try:\n",
    "            nexturl = driver.find_element_by_link_text(\"Next\")\n",
    "            print \"NEXT BUTTON\", nexturl\n",
    "        except NoSuchElementException:\n",
    "            print \"NO LINK\"\n",
    "            break\n",
    "        nexturl.click() \n",
    "        body = driver.find_element_by_tag_name(\"body\")\n",
    "        body.send_keys(Keys.CONTROL + 't')\n",
    "        def link_has_gone_stale():\n",
    "            try:\n",
    "                # poll the link with an arbitrary call\n",
    "                nexturl.find_elements_by_id('doesnt-matter') \n",
    "                return False\n",
    "            except StaleElementReferenceException:\n",
    "                return True\n",
    "        time.sleep(1)\n",
    "        wait_for(link_has_gone_stale)\n",
    "        \n",
    "        nextpage = reviewsoup.findAll(\"a\",{\"class\":\"pageNum taLnk\"})\n",
    "        print \"go to next\", nextpage[0]['href']\n",
    "        revurl = nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "    #omnidict[id] = ratedict\n",
    "    #print id,\":\",k,\":\", v\n",
    "    #print stars\n",
    "def wait_for(condition_function):\n",
    "    start_time = time.time()\n",
    "    while time.time() < start_time + 10:\n",
    "        if condition_function():\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "    raise Exception(\n",
    "        'Timeout waiting for {}'.format(condition_function.__name__)\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "#globalurl = \"/Hotel_Review-g60745-d89599-Reviews-or5270-Omni_Parker_House-Boston_Massachusetts.html#REVIEWS\"\n",
    "print 'get url'\n",
    "omnifile = open(\"omni-scrapte-out.dat\",\"w\")\n",
    "ratingfile = open(\"travel-rating.dat\",\"w\")\n",
    "debugfile = open(\"debug.log\",\"w\")\n",
    "tourism_url = get_tourism_page('boston', 'massachusetts')\n",
    "#Get URL to obtaint the list of hotels in a specific city\n",
    "city_url = get_city_page(tourism_url)\n",
    "c=0\n",
    "#driver = webdriver.Firefox()\n",
    "#driver.wait = WebDriverWait(driver, 5)\n",
    "while(True):\n",
    "    c +=1\n",
    "    html = get_hotellist_page(city_url,c)\n",
    "    city_url = parse_hotellist_page(html)\n",
    "    if not city_url:\n",
    "        break\n",
    "omnifile.close()\n",
    "ratingfile.close()\n",
    "debugfile.close()\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2 (20 pts) **\n",
    "\n",
    "Now, we will use regression to analyze this information. First, we will fit a linear regression model that predicts the average rating. For example, for the hotel above, the average rating is\n",
    "\n",
    "$$ \\text{AVG_SCORE} = \\frac{1*31 + 2*33 + 3*98 + 4*504 + 5*1861}{2527}$$\n",
    "\n",
    "Use the model to analyze the important factors that decide the $\\text{AVG_SCORE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 3 (30 pts) **\n",
    "\n",
    "Finally, we will use logistic regression to decide if a hotel is _excellent_ or not. We classify a hotel as _excellent_ if more than **60%** of its ratings are 5 stars. This is a binary attribute on which we can fit a logistic regression model. As before, use the model to analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
