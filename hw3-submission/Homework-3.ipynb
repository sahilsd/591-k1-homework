{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Hotel Ratings on Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will focus on practicing two techniques: web scraping and regression. For the first part, we will get some basic information for each hotel in Boston. Then, we will fit a regression model on this information and try to analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1 (30 pts)**\n",
    "\n",
    "We will scrape the data using Beautiful Soup. For each hotel that our search returns, we will get the information below.\n",
    "\n",
    "![Information to be scraped](hotel_info.png)\n",
    "\n",
    "Of course, feel free to collect even more data if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " get url\n",
      "URL TO REQUEST: http://www.tripadvisor.com/TypeAheadJson?query=boston%20massachusetts&action=API\n",
      "RESULTS:  {u'lookbackServlet': None, u'name': u'Boston, Massachusetts, United States', u'data_type': u'LOCATION', u'title': u'Destinations', u'url': u'/Tourism-g60745-Boston_Massachusetts-Vacations.html', u'value': 60745, u'coords': u'42.357277,-71.05834', u'urls': [{u'url': u'/Tourism-g60745-Boston_Massachusetts-Vacations.html', u'type': u'GEO', u'name': u'Boston Tourism', u'url_type': u'geo'}], u'scope': u'global', u'type': u'GEO'}\n",
      "CITY PAGE URL: /Hotels-g60745-Boston_Massachusetts-Hotels.html\n",
      "Hotel page 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ae3d76d11138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hotellist_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcity_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m     \u001b[0mcity_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_hotellist_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m     \u001b[1;31m#if not city_url:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-ae3d76d11138>\u001b[0m in \u001b[0;36mparse_hotellist_page\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;31m# Extract hotel name, star rating and number of reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mhotel_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;34m'listing easyClear  p13n_imperfect '\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhotel_box\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhotel_boxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhotel_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;34m'listing_title'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36mfindAll\u001b[1;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_findAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindAll\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36m_findAll\u001b[1;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;31m# If it's text, make sure the text matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36msearchTag\u001b[1;34m(self, markupName, markupAttrs)\u001b[0m\n\u001b[0;32m    937\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkupAttrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m                                 \u001b[0mmarkupAttrMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m                     \u001b[0mattrValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkupAttrMap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatchAgainst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m                         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mgiven\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;34m'default'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         attribute.\"\"\"\n\u001b[1;32m--> 594\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getAttrMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36m_getAttrMap\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    865\u001b[0m         \"\"\"Initializes a map representation of this tag's attributes,\n\u001b[0;32m    866\u001b[0m         if not already initialized.\"\"\"\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attrMap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrMap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, tag)\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(self, name, attrs, recursive, text, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m         criteria.\"\"\"\n\u001b[0;32m    828\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36mfindAll\u001b[1;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_findAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindAll\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36m_findAll\u001b[1;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;31m# If it's text, make sure the text matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36msearchTag\u001b[1;34m(self, markupName, markupAttrs)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                \u001b[1;32mor\u001b[0m \u001b[0mcallFunctionWithTagData\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m                \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mmarkup\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkupName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallFunctionWithTagData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sahil/anaconda2/lib/python2.7/site-packages/BeautifulSoup.pyc\u001b[0m in \u001b[0;36m_matches\u001b[1;34m(self, markup, matchAgainst)\u001b[0m\n\u001b[0;32m    995\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatchAgainst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__iter__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# list-like\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatchAgainst\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatchAgainst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'items'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatchAgainst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmatchAgainst\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "import collections\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "base_url = \"http://www.tripadvisor.com\"\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\"\n",
    "\n",
    "\"\"\" STEP 1  \"\"\"\n",
    "def get_tourism_page(city, state):\n",
    "    \"\"\" \n",
    "        Return the json containing the\n",
    "        URL of the tourism city page\n",
    "    \"\"\"\n",
    "\n",
    "    # EXAMPLE: http://www.tripadvisor.com/TypeAheadJson?query=boston%20massachusetts&action=API\n",
    "    #          http://www.tripadvisor.com//TypeAheadJson?query=san%20francisco%20california&type=GEO&action=API\n",
    "    url = \"%s/TypeAheadJson?query=%s%%20%s&action=API\" % (base_url, \"%20\".join(city.split()), state)\n",
    "    print \"URL TO REQUEST:\", url\n",
    "    \n",
    "    # Given the url, request the HTML page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "\n",
    "    # Save to file\n",
    "    #with open('search-page.json', \"w\") as h:\n",
    "        #h.write(html)\n",
    "\n",
    "    # Parse json to get url\n",
    "    js = json.loads(html)\n",
    "    results = js['results']\n",
    "    print \"RESULTS: \", results[0]\n",
    "    urls = results[0]['urls'][0]\n",
    "\n",
    "    # get tourism page url\n",
    "    tourism_url = urls['url']\n",
    "    return tourism_url\n",
    "\n",
    "\"\"\" STEP 2  \"\"\"\n",
    "def get_city_page(tourism_url):\n",
    "    \"\"\" \n",
    "        Get the URL of the hotels of the city\n",
    "        using the URL returned by the function\n",
    "        get_tourism_page()\n",
    "    \"\"\"\n",
    "\n",
    "    url = base_url + tourism_url\n",
    "\n",
    "    # Given the url, request the HTML page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    \n",
    "    # Save to file\t\n",
    "    #with open('-tourism-page.html', \"w\") as h:\n",
    "        #h.write(html)\n",
    "\n",
    "\n",
    "    # Use BeautifulSoup to extract the url for the list of hotels in \n",
    "    # the city and state we are interested in.\n",
    "    # For exampel in this case we need to  \n",
    "    #<li class=\"hotels twoLines\">\n",
    "    #<a href=\"/Hotels-g60745-Boston_Massachusetts-Hotels.html\" data-trk=\"hotels_nav\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    li = soup.find(\"li\", {\"class\": \"hotels twoLines\"})\n",
    "    city_url = li.find('a', href = True)\n",
    "    print \"CITY PAGE URL:\", city_url['href']\n",
    "    return city_url['href']\n",
    "\n",
    "\n",
    "\"\"\" STEP 3 \"\"\"\n",
    "def get_hotellist_page(city_url, count):\n",
    "    \"\"\" Get the hotel list page given the url returned by\n",
    "        get_city_page(). Return the html after saving\n",
    "        it to the datadir \n",
    "    \"\"\"\n",
    "    print \"Hotel page\", count\n",
    "    url = base_url + city_url\n",
    "    # Sleep 2 sec before starting a new http request\n",
    "    time.sleep(2)\n",
    "    # Request page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    # Save the \n",
    "    #with open('boston-hotelist-' + str(count) + '.html', \"w\") as h:\n",
    "        #h.write(html)\n",
    "    return html\n",
    "\n",
    "\"\"\" STEP 4 \"\"\"\n",
    "def parse_hotellist_page(html):\n",
    "    \"\"\" \n",
    "    Parse the html pages returned by get_hotellist_page().\n",
    "    Return the next url page to scrape (a city can have\n",
    "    more than one page of hotels) if there is, else exit\n",
    "    the script.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "# Extract hotel name, star rating and number of reviews\n",
    "    hotel_boxes = soup.findAll('div', {'class' :'listing easyClear  p13n_imperfect '})\n",
    "    for hotel_box in hotel_boxes:\n",
    "        name = hotel_box.find('div', {'class' :'listing_title'}).find(text=True)\n",
    "        try:\n",
    "            rating = hotel_box.find('div', {'class' :'listing_rating'})\n",
    "            reviews = rating.find('span', {'class' :'more'}).find(text=True)\n",
    "            stars = hotel_box.find(\"img\", {\"class\" : \"sprite-ratings\"})\n",
    "        except Exception, e:\n",
    "            log.error(\"No ratings for this hotel\")\n",
    "            reviews = \"N/A\"\n",
    "            stars = 'N/A'\n",
    "        hotelref = hotel_box.findAll('a', href= True)\n",
    "        #print \"go to \", hotelref[0]['href'],\" and get traveler ratings\"\n",
    "        ratingfile.write(\"++NEW HOTEL++ %s\\n\" % name)\n",
    "        print '.',\n",
    "        getTraverlerRating(hotelref[0]['href'])\n",
    "        \n",
    "        \n",
    "        if stars != 'N/A':\n",
    "            #log.info(\"Stars: %s\" % stars['alt'].split()[0])\n",
    "            stars = stars['alt'].split()[0]\n",
    "        if name == \"Omni Parker House\":\n",
    "            print \"Found Omni Parker House. Scrape reviews\"\n",
    "            print \"HOTEL NAME:\", name\n",
    "            print \"HOTEL REVIEWS: \", reviews\n",
    "            print \"HOTEL STAR RATING:\", stars\n",
    "            omnihrefs = hotel_box.findAll('a', href= True)\n",
    "            for omnihref in omnihrefs:\n",
    "                #print omnihref, \"######\", omnihref['href']\n",
    "                if omnihref.find(text = True) == 'Omni Parker House':\n",
    "                    \n",
    "                    pg = 0\n",
    "                    #print \"Review url is\", omnihref['href']\n",
    "                    print \"page #\", pg,\n",
    "                    ret = scrapeReview(omnihref['href'], pg)\n",
    "                    #ret = scrapeFaster(omnihref['href'])                    \n",
    "                    while ret:\n",
    "                        pg +=1\n",
    "                        print \"page #\", pg, \n",
    "                        ret = scrapeReview(ret, pg)\n",
    "                        \n",
    "                    \"\"\"\n",
    "                    print \"Review url begin:\", omnihref['href'] \n",
    "                    scrapeReview(omnihref['href'])\n",
    "                    \"\"\"\n",
    "                    #add this block in main flow to scrape everything\n",
    "                #return\n",
    "\n",
    "# # Get next URL page if exists, else exit\n",
    "    div = soup.find(\"div\", {\"class\" : \"unified pagination standard_pagination\"})\n",
    "    # check if last page\n",
    "    if div.find('span', {'class' : 'nav next ui_button disabled'}):\n",
    "        print \"We reached last page\"\n",
    "        return None\n",
    "    # If it is not las page there must be the Next URL\n",
    "    hrefs = div.findAll('a', href= True)\n",
    "    for href in hrefs:\n",
    "        if href.find(text = True) == 'Next':\n",
    "            print \"Next url is\", href['href']\n",
    "            return href['href']\n",
    "\n",
    "\"\"\"Get Traverler's ratings for every hotel\"\"\"\n",
    "def getTraverlerRating(hotelurl):\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+hotelurl, headers=headers)\n",
    "    #print response\n",
    "    html = response.text.encode('utf-8')   \n",
    "    hotelsoup = BeautifulSoup(html)\n",
    "    \n",
    "    filterbox = hotelsoup.findAll(\"div\",{\"class\":\"with_histogram\"})\n",
    "    ratebox = filterbox[0].findAll(\"div\",{\"class\":\"col rating \"})\n",
    "    ratinglist = ratebox[0].findAll(\"li\")\n",
    "    excel = ratinglist[0].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_5\"})[0]\n",
    "    ratingfile.write(\"Excellent:%s\\n\" % excel.findAll(\"span\")[2].find(text=True))\n",
    "    #print \"excel\", excel.findAll(\"span\")[2].find(text=True), \n",
    "    vgood = ratinglist[1].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_4\"})[0]\n",
    "    ratingfile.write(\"Very good: %s\\n\" % vgood.findAll(\"span\")[2].find(text=True))\n",
    "    \n",
    "    avg = ratinglist[2].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_3\"})[0]\n",
    "    ratingfile.write(\"Average:%s\\n\" % avg.findAll(\"span\")[2].find(text=True))\n",
    "    \n",
    "    poor = ratinglist[3].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_2\"})[0]\n",
    "    ratingfile.write(\"Poor:%s\\n\" % poor.findAll(\"span\")[2].find(text=True))\n",
    "    \n",
    "    terrible = ratinglist[4].findAll(\"label\",{\"for\":\"taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_1\"})[0]\n",
    "    ratingfile.write(\"Terrible:%s\\n\" % terrible.findAll(\"span\")[2].find(text=True))\n",
    "    \n",
    "    #sys.exit()\n",
    "        \n",
    "\"\"\"STEP 5: Go through each review\"\"\"   \n",
    "\"\"\"\n",
    "def scrapeFaster(url):\n",
    "    driver.get(base_url+url)\n",
    "    \n",
    "    pagehtml = driver.page_source\n",
    "    pgsoup = BeautifulSoup(pagehtml)\n",
    "    try:\n",
    "        nexturl = driver.find_element_by_link_text(\"More\")\n",
    "        print \"More BUTTON\", nexturl\n",
    "    except NoSuchElementException:\n",
    "        print \"NO LINK\"\n",
    "        return\n",
    "    nexturl.click() \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    print \"page loaded\"\n",
    "\"\"\"    \n",
    "\n",
    "\n",
    "def scrapeReview(reviewurl, pgnum):\n",
    "    #return\n",
    "    #print base_url+reviewurl\n",
    "    debugfile.write(\"\\nscrapeReview: url %s,\" % base_url+reviewurl)\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+reviewurl, headers=headers)\n",
    "    #print response\n",
    "    debugfile.write(\"scrapeReview: response %s\\n\" % response)\n",
    "    html = response.text.encode('utf-8')   \n",
    "    reviewsoup = BeautifulSoup(html) \n",
    "    \n",
    "    revbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector   track_back\"})\n",
    "    olderrevbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector  \"})\n",
    "    oldestbox = reviewsoup.findAll(\"div\", {\"class\":\"reviewSelector  first_aph   track_back\"})\n",
    "    \n",
    "    #if len(olderrevbox):\n",
    "        #print \"older reviews\", len(olderrevbox)\n",
    "    debugfile.write(\"scrapeReview: total reviews to be parsed %s\\n\" % str(len(revbox)+len(olderrevbox)+len(oldestbox)))\n",
    "    print \"(\",len(revbox)+len(olderrevbox)+len(oldestbox),\")|\",\n",
    "    pg = 1\n",
    "    revbox += olderrevbox+oldestbox\n",
    "    \n",
    "    #click on more button and send expanded cells to getstars2 one-by-one?\n",
    "    for r in revbox:\n",
    "        reviews = r.findAll('a', href=True)        \n",
    "        for rev in reviews:            \n",
    "            thisrevurl = rev['href']\n",
    "            #print thisrevurl\n",
    "            #now make http request for review url and write values to a file\n",
    "            getStars2(thisrevurl)\n",
    "    \"\"\"\n",
    "    reviews = revbox[0].findAll('a', href=True)\n",
    "    thisurl = reviews[0]['href']\n",
    "    nextpageret = getStars(thisurl,0)\n",
    "    \"\"\"\n",
    "    \n",
    "    #nextpages = reviewsoup.findAll(\"a\", {\"class\":\"pageNum taLnk\"})\n",
    "    \"\"\"\n",
    "    print \"next review page\", nextpageret\n",
    "    while nextpageret:        \n",
    "        pg +=1\n",
    "        print \"Next review page #\", pg        \n",
    "        #nextpageret = getStars(nextpageret, pg)\n",
    "    \"\"\"\n",
    "    nextpages = reviewsoup.findAll(\"a\", {\"class\":\"pageNum taLnk\"})\n",
    "    pgnum = min(pgnum,4)\n",
    "    try:\n",
    "        #print \"\\nnext page?\", nextpages[pgnum]['href']\n",
    "        debugfile.write(\"scrapeReview: next page url %s\\n\" % nextpages[pgnum]['href'])\n",
    "        return nextpages[pgnum]['href']\n",
    "    except IndexError:\n",
    "        print \"Done with all pages\", pgnum\n",
    "        return None\n",
    "\n",
    "\"\"\"STEP 6 : Access individual review, parse ratings and store in a file\"\"\"\n",
    "def getStars2(revurl):\n",
    "    #print base_url+revurl,\n",
    "    debugfile.write(\"getStars2: url %s,\" % base_url+revurl)\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(base_url+revurl, headers=headers)\n",
    "    #print response\n",
    "    debugfile.write(\"getStars2: response %s\\n\" % response)\n",
    "    html = response.text.encode('utf-8') \n",
    "    \n",
    "    reviewsoup = BeautifulSoup(html)\n",
    "    reviewblock = reviewsoup.findAll(\"div\",{\"class\":\"deckC\"})\n",
    "    try:\n",
    "        reviewlist = reviewblock[0].findAll(\"div\",{\"class\":\"  reviewSelector \"})\n",
    "    except IndexError:\n",
    "        return\n",
    "    review = reviewlist[0]\n",
    "    \n",
    "            \n",
    "    #print review\n",
    "    id = review['id']\n",
    "    #print id, \n",
    "    debugfile.write(\"getStars2: id: %s\\t\" % id)\n",
    "    ratelist = review.findAll(\"div\", {\"class\":\"rating-list\"})\n",
    "    #print ratelist\n",
    "    stars = ratelist[0].findAll(\"li\",{\"class\":\"recommend-answer\"})\n",
    "    #inside stars, access sprite and description and write\n",
    "    #print stars\n",
    "    #ratedict = collections.defaultdict(list)\n",
    "    for val in stars:\n",
    "        v = val.findAll(\"img\")\n",
    "        k = val.findAll(\"div\",{\"class\":\"recommend-description\"})\n",
    "        #print k,v\n",
    "        #print id, \":\",k[0].find(text=True),\":\", v[0]['alt'][0]\n",
    "        #ratedict[k[0].find(text=True)] = v[0]['alt'][0]\n",
    "\n",
    "        omnifile.write(\"%s:\" % id)\n",
    "        omnifile.write(\"%s:\" % k[0].find(text=True))\n",
    "        omnifile.write(\"%s\\n\" % v[0]['alt'][0])\n",
    "    nextpage = reviewsoup.findAll(\"a\",{\"class\":\"pageNum taLnk\"})\n",
    "    #print \"go to next\", nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "\"\"\"\n",
    "def getStars(revurl, pg):\n",
    "    print base_url+revurl\n",
    "    driver.get(base_url+revurl)\n",
    "    print \"page #\", pg\n",
    "    #time.sleep(1)\n",
    "    while True:\n",
    "        #geturl = base_url+revurl\n",
    "        #headers = { 'User-Agent' : user_agent }\n",
    "        #response = requests.get(base_url+revurl, headers=headers)\n",
    "        #print response\n",
    "        #html = response.text.encode('utf-8')   \n",
    "        \n",
    "        html = driver.page_source\n",
    "        reviewsoup = BeautifulSoup(html) \n",
    "        reviewblock = reviewsoup.findAll(\"div\",{\"class\":\"deckC\"})\n",
    "        reviewlist = reviewblock[0].findAll(\"div\",{\"class\":\"  reviewSelector \"})\n",
    "        #print reviewlist\n",
    "\n",
    "        revnum = 0\n",
    "        for review in reviewlist:        \n",
    "            \n",
    "            if pg and not revnum:\n",
    "                revnum += 1\n",
    "                continue\n",
    "            \n",
    "            #print review\n",
    "            id = review['id']\n",
    "            print id\n",
    "            ratelist = review.findAll(\"div\", {\"class\":\"rating-list\"})\n",
    "            #print ratelist\n",
    "            for i in xrange(len(ratelist)):\n",
    "\n",
    "                stars = ratelist[i].findAll(\"li\",{\"class\":\"recommend-answer\"})\n",
    "                #inside stars, access sprite and description and write\n",
    "                #print stars\n",
    "                #ratedict = collections.defaultdict(list)\n",
    "                for val in stars:\n",
    "                    v = val.findAll(\"img\")\n",
    "                    k = val.findAll(\"div\",{\"class\":\"recommend-description\"})\n",
    "                    #print k,v\n",
    "                    #print id, \":\",k[0].find(text=True),\":\", v[0]['alt'][0]\n",
    "                    #ratedict[k[0].find(text=True)] = v[0]['alt'][0]\n",
    "\n",
    "                    #omnifile.write(\"%s:\" % id)\n",
    "                    #omnifile.write(\"%s:\" % k[0].find(text=True))\n",
    "                    #omnifile.write(\"%s\\n\" % v[0]['alt'][0])\n",
    "            revnum += 1\n",
    "            \n",
    "        pg += 1\n",
    "        try:\n",
    "            nexturl = driver.find_element_by_link_text(\"Next\")\n",
    "            print \"NEXT BUTTON\", nexturl\n",
    "        except NoSuchElementException:\n",
    "            print \"NO LINK\"\n",
    "            break\n",
    "        nexturl.click() \n",
    "        body = driver.find_element_by_tag_name(\"body\")\n",
    "        body.send_keys(Keys.CONTROL + 't')\n",
    "        def link_has_gone_stale():\n",
    "            try:\n",
    "                # poll the link with an arbitrary call\n",
    "                nexturl.find_elements_by_id('doesnt-matter') \n",
    "                return False\n",
    "            except StaleElementReferenceException:\n",
    "                return True\n",
    "        time.sleep(1)\n",
    "        wait_for(link_has_gone_stale)\n",
    "        \n",
    "        nextpage = reviewsoup.findAll(\"a\",{\"class\":\"pageNum taLnk\"})\n",
    "        print \"go to next\", nextpage[0]['href']\n",
    "        revurl = nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "    #return nextpage[0]['href']\n",
    "    #omnidict[id] = ratedict\n",
    "    #print id,\":\",k,\":\", v\n",
    "    #print stars\n",
    "def wait_for(condition_function):\n",
    "    start_time = time.time()\n",
    "    while time.time() < start_time + 10:\n",
    "        if condition_function():\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "    raise Exception(\n",
    "        'Timeout waiting for {}'.format(condition_function.__name__)\n",
    "    )\n",
    "\"\"\"\n",
    "    \n",
    "print 'get url'\n",
    "omnifile = open(\"omni-scrapte-out.dat\",\"w\")\n",
    "ratingfile = open(\"travel-rating.dat\",\"w\")\n",
    "debugfile = open(\"debug.log\",\"w\")\n",
    "tourism_url = get_tourism_page('boston', 'massachusetts')\n",
    "#Get URL to obtaint the list of hotels in a specific city\n",
    "city_url = get_city_page(tourism_url)\n",
    "c=0\n",
    "#driver = webdriver.Firefox()\n",
    "#driver.wait = WebDriverWait(driver, 5)\n",
    "while(True):\n",
    "    c +=1\n",
    "    html = get_hotellist_page(city_url,c)\n",
    "    city_url = parse_hotellist_page(html)\n",
    "    #if not city_url:\n",
    "    break\n",
    "omnifile.close()\n",
    "ratingfile.close()\n",
    "debugfile.close()\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2 (20 pts) **\n",
    "\n",
    "Now, we will use regression to analyze this information. First, we will fit a linear regression model that predicts the average rating. For example, for the hotel above, the average rating is\n",
    "\n",
    "$$ \\text{AVG_SCORE} = \\frac{1*31 + 2*33 + 3*98 + 4*504 + 5*1861}{2527}$$\n",
    "\n",
    "Use the model to analyze the important factors that decide the $\\text{AVG_SCORE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 3 (30 pts) **\n",
    "\n",
    "Finally, we will use logistic regression to decide if a hotel is _excellent_ or not. We classify a hotel as _excellent_ if more than **60%** of its ratings are 5 stars. This is a binary attribute on which we can fit a logistic regression model. As before, use the model to analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
