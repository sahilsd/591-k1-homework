{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Since purchase prediction can either be only one or zero, I've treated this as a classification problem. Simplest way was to populate train.json as bought and some other random pairs as not bought. For each such user and item, I took percentile value using logic similar to baseline code by enumerating most popular elements. I used regression/classification on these vectors formed by these [bought or not] v/s [user and item rank] and used predict() method to get which class testing data will belong to.\n",
    "    \n",
    "    Here I found SGD classifier to be sufficiently fast and enough to get a top 10 entry in Kaggle. I have used 500,000 more random pairs as not bought along with 1,000,000 bought training entires.\n",
    "    \n",
    "    REF http://scikit-learn.org/stable/modules/sgd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "from __future__ import division\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from baseline\n",
    "\n",
    "def readJson(f):\n",
    "  for l in open(f):\n",
    "    yield eval(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read training data - accessed multiple times\n",
    "\n",
    "train = []\n",
    "\n",
    "for l in readJson(\"train.json\"):\n",
    "    train.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating 5.44251012802\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#parse lines from training file into dictionaries\n",
    "training = []\n",
    "start = time.time()\n",
    "\n",
    "userset = set([])\n",
    "itemset = set([])\n",
    "trainset = set([])\n",
    "\n",
    "for l in train:\n",
    "    \"\"\"\n",
    "    All bought items\n",
    "    \"\"\"\n",
    "    training.append((l['reviewerID']+\"-\"+l['asin'], 1))\n",
    "    \n",
    "    userset.add(l['reviewerID'])\n",
    "    itemset.add(l['asin'])\n",
    "    trainset.add((l['reviewerID'],l['asin']))\n",
    "    \n",
    "\n",
    "userlist = list(userset)\n",
    "itemlist = list(itemset)\n",
    "\"\"\"\n",
    "Get more unbought data\n",
    "\"\"\"\n",
    "resize = 1.5*len(training)\n",
    "while len(training) < resize:\n",
    "    u = userlist[random.randrange(0, len(userlist))]\n",
    "    i = itemlist[random.randrange(0, len(itemlist))]\n",
    "    \n",
    "    if (u,i) not in trainset:\n",
    "        trainset.add((u,i))\n",
    "        training.append((u+\"-\"+i, 0))\n",
    "\n",
    "print \"Done creating\", time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Populate users and items in reverse sorted order which can be later used to calculate percentiles for both and then append to X of regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bc19970fff3383b2fe947cf9a3a5d7b13b6e57ef2cd53abc52bb2dfedf5fb1cd-a6ed402934e3c1138111dce09256538afb04c566edf37c16b9ba099d23afb764', 1)\n",
      "Done 171185\n",
      "509678 171185\n"
     ]
    }
   ],
   "source": [
    "#same as baseline - popularity to get %le\n",
    "\n",
    "itemCount = defaultdict(int)\n",
    "userCount = defaultdict(int)\n",
    "print training[0]\n",
    "for (ui,bought) in training:\n",
    "    userid,itemid = ui.strip().split('-')\n",
    "    \n",
    "    if itemid not in itemCount:\n",
    "        itemCount[itemid]=0\n",
    "    if userid not in userCount:\n",
    "        userCount[userid]=0  \n",
    "    \n",
    "    if bought==1:\n",
    "        userCount[userid] += 1\n",
    "        itemCount[itemid] += 1\n",
    "\n",
    "\n",
    "mostPopularUsers=[]\n",
    "mostPopularUsers = [(userCount[x], x) for x in userCount]\n",
    "mostPopularUsers.sort()\n",
    "mostPopularUsers.reverse()\n",
    "    \n",
    "mostPopularItems=[]\n",
    "mostPopularItems = [(itemCount[x], x) for x in itemCount]\n",
    "mostPopularItems.sort()\n",
    "mostPopularItems.reverse()\n",
    "\n",
    "print \"Done\", len(mostPopularItems)\n",
    "print len(mostPopularUsers), len(mostPopularItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "Done itemrank and userrank list 171185 509678\n"
     ]
    }
   ],
   "source": [
    "#rankings wrt %le - use count and increment percentile by 0.05\n",
    "\n",
    "ItemPerc = defaultdict(list)\n",
    "UserPerc = defaultdict(list)\n",
    "\n",
    "percentile = 0.05\n",
    "for i in range(0,len(mostPopularItems)):\n",
    "    (count, item)= mostPopularItems[i]\n",
    "    if i>len(mostPopularItems)*percentile:\n",
    "        percentile+=.05\n",
    "    ItemPerc[item]=percentile\n",
    "print percentile\n",
    "\n",
    "percentile = 0.05\n",
    "for i in range(0,len(mostPopularUsers)):\n",
    "    (count, user)= mostPopularUsers[i]\n",
    "    if i>len(mostPopularUsers)*percentile:\n",
    "        percentile+=.05\n",
    "    UserPerc[user]=percentile\n",
    "    \n",
    "print percentile\n",
    "print \"Done itemrank and userrank list\", len(ItemPerc), len(UserPerc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bc19970fff3383b2fe947cf9a3a5d7b13b6e57ef2cd53abc52bb2dfedf5fb1cd-a6ed402934e3c1138111dce09256538afb04c566edf37c16b9ba099d23afb764', 1)\n"
     ]
    }
   ],
   "source": [
    "#logical reg vectors create X and y\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "print training[0]\n",
    "for (ui, bought) in training:\n",
    "    u,i = ui.strip().split('-')\n",
    "    elem=[ItemPerc[i],UserPerc[u]]\n",
    "    \n",
    "    y.append(bought)\n",
    "    X.append(elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.4219724  -1.95474662]]\n"
     ]
    }
   ],
   "source": [
    "#logical reg\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X, y) #UNCOMMENT ME\n",
    "print logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Use Huber loss with epsilon=0.5 gave best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.6,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='huber', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USE ME\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "sgd = linear_model.SGDClassifier(alpha=0.001, n_iter=5, epsilon=0.6, loss=\"huber\")\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict using regression \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "predfile = open(\"CLFpredictions_Purchase.txt\", 'w')\n",
    "predfile.write(\"reviewerID-asin,prediction\\n\")\n",
    "\n",
    "for l in open(\"pairs_Purchase.txt\"):\n",
    "    if l.startswith(\"reviewer\"):\n",
    "        continue\n",
    "        \n",
    "    u,i = l.strip().split('-')\n",
    "    uP = UserPerc[u]\n",
    "    iP = ItemPerc[i]\n",
    "    \n",
    "    if uP == []:\n",
    "        uP=1\n",
    "    if iP == []:\n",
    "        iP = 1\n",
    "                \n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "    if (sgd.predict([uP, iP])==[0]):\n",
    "        predfile.write(u + '-' + i + \",0\\n\")\n",
    "    else:\n",
    "        predfile.write(u + '-' + i + \",1\\n\")\n",
    "\n",
    "predfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes a lot of time\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "#clf.fit(X, y)  #UNCOMMENT ME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
